,created_utc,author,score,title,selftext
0,2017-01-18,asumm33rs,0,So am I headed for failure?? Help/Advice Needed,"             Hi guys. I'm pretty new to the investing world about a few months now, and after discovering robo-trade ( acorns, betterment etc.) I decided to set out on my own with a popular "" free"" trading app. I'm still in the midst of learning of course but I think I might've done a big boo boo, and this is why I'm asking for genuine advice. 
            
               Immersing myself into learning the trade and listening to this report about this great stock..And this momentous other stock I've slowly  amassed a portfolio of about 30 different companies. Lol yeah I know...I do need help..But this is just the half of it. I'm a student so since I really couldn't afford much of this and that, I would buy two here maybe four there, and now this portfolio is at about 10 grand with 30 different companies. I don't know if this is correct , I mean is there a good number to be diversified? Am I loosing money when a stock rises and I only have 2-4 shares? Or is it safe for now..I guess a part of me wants to succeed at this so badly that I'm tryin ng to touch everywhere so I don't loose any...but that's impossible and to be honest I felt like it was okay at first..Hey what's two shares, but the trickle has led to the pond and now I'm at ten grand. 
              
           I need to start thinking about growth,and returns..And if a company does grow or yields a nice return why only two and now the whole amount. My plan lately has been to do deep research into these companies and choose a handful to reorganize my portfolio..is that bad? I feel in some sense some weird addiction into having the best..Or latest in my portfolio..Even if it is two shares..lol ...Sigh...So..Advice... 
                              Sincerely
                      Lost at the Market!"
1,2017-01-10,Young_Yolo_Baggings,0,Why Bottom-up Value Investors need systems Thinking,"(I stole this from Samir of Askeladden Capital- I formatted it entirely by myself since /r/investing doesn't allow links - but if you wanna read it in it with footnotes, sources and original formatting - here is a link to another investors forum: [Link](https://www.hvst.com/public-pages/askeladden-capital-management/posts/79186-why-value-investors-need-systems-thinking))

Stop me if you’ve heard this one before: 

an NFL team is building ​state-of-the-art new digs,
outclassing the other 31 with bold architecture and whiz-bang digital scoreboards massive enough to render the actual playing field into a distracting sideshow. The team solicits taxpayer funding by projecting how the venue will reinvigorate the local economy. How could you say no? It’s a matter
of local pride to be the biggest and best; now it’s apparently profitable to boot.

Except by the time the project’s shovel-ready, the stadium’s already been eclipsed by another two in
the works - one by a bitter division rival, no less. Fast-forward a decade or three, and all of a sudden home field is looking pretty sad and worn down. The team freshens up its now-vintage
pitchbook and sticks out an oversized mascot paw for more bond money…

//

I’m the sort of investor who isn’t terribly popular at dinner parties, on the rare occasion I end up at
them, because everyone’s first question is ​“oh, where do you think ​ the market’s going?” Naturally,
I reply with the standard bottom-up value-oriented line: ​“you know, I really don’t have any useful
thoughts on the market, but I could tell you about this awesome new product from my favorite
corporate training company that totally changes how Chief Learning Officers create impact
journeys for...” Lo and behold, I look up, and they’ve abandoned me for the lobster samosas,
leaving me to drown my unfinished sentence in a cold Coca-Cola.

Nonetheless, while I focus on picking strong swimmers rather than predicting the tide, one
potential mistake that can be made when deeply analyzing individual opportunities (in addition to
over-reliance on ​exhaustive data collection rather than thoughtful decision making) is to get so
wrapped up in company-specific details that you overlook the broader systemic context - the “and
then what?” analytical step out of the reductionistically precise comfort of a spreadsheet into the
messy, complex real world.

Here’s a quick example of how you could be lulled into a false sense of bottom-up security that
conflicts with fairly obvious system-level issues. This time last year, for reasons I can’t quite
remember, I actually spent a little bit of time working on brick-and-mortar retailers. (I know, I
know - not one of my better time allocation decisions.) 

The first one I looked at had an interesting
story: for what management assured investors were fully controllable reasons around product
assortment, the comp stack didn’t look super, but they had a magic plan: ​investing in e-commerce,
reducing time-to-market to emulate fast-fashion peers, and implementing new real-time data
systems to target customers and “chase” hot products flying off the shelves.

Oh cool - that seems like a reasonable strategy. Or at least it did, until I looked at retailers #2, 3,
and 4… and discovered that, whether due to social proof or some other factor, they were all taking
pretty much the same approach.

My mind automatically jumped to the system level - unit-wise, the apparel TAM is more or less a
zero-sum pie, insofar as anyone not named “Dez Bryant” or “your wife” is only going to buy ​so
many pairs of ​ shoes between now and next Christmas. ( ​So, so many.) Quips aside, retail’s
obviously a mature market where achieving robust growth necessitates either differentiated
product to the extent of pricing power, or stealing volume from competitors. 

Mathematically, share gains by LULU/Gildan/etc necessitate commensurate share losses by Levi’s/Hanes/etc.
Implementing real-time sales tracking, a more robust e-commerce platform, and a supply chain
with shorter lead times certainly seems like a valid competitive strategy with a reasonable shot at
success… until you figure out all your competitors are pursuing the same approach, which means,
on net, any one player will achieve zero incremental competitive advantage.

With another year in the books, my suspicions were confirmed - new data systems couldn’t salvage
comps for any of those sad-sack retailers. Admittedly you can’t blame retail executives for trying;
after all, if they didn’t make those moves, they might be holding their next earnings call on their
way up the steps of bankruptcy court. However, investors blinded by “cheap” valuation metrics,
hoping for improving comps and margins, should’ve known better. 

Value trap 101. Of course, I’m not breaking any new intellectual ground here - this is exactly why Buffett stopped
buying cigar butts with no competitive advantage. Keeping with the apparel theme, here is a pithy
explanation from Charlie Munger:

*“[in a commodity industry], all the advantages from great improvements [will] flow through to the
customers. The people who sell the machinery - and by and large, even the internal bureaucrats
urging you to buy the equipment - show you projections with the amount you’ll save at current
prices with the new technology. However, they don’t do the second step of the analysis - which is to
determine how much [will] stay home and how much [will] just flow through to the customer.

I’ve never seen a single projection incorporating that second step in my life. And I see [such
projections] all the time. Rather, they always read: “This capital outlay will save you so much
money that it will pay for itself in three years.”*

So you keep buying things that will pay for themselves in three years. And after 20 years of doing it, somehow you’ve earned a return of only about 4% per annum. That’s the textile business.” 

A third and final example, coincidentally also retail-related, is the bull thesis on Seritage Growth
Properties (SRG) - i.e. slicing up and redeveloping erstwhile Sears boxes at substantially higher
rental rates.

I’ve seen multiple detailed writeups, none of which addressed my basic system-level
question: ​which tenants can generate sustainable profits in B/C space? Fellow big-box anchor
J.C. Penney has issues, the venerable Macy’s is slashing jobs, and I don’t think new Barnes and
Nobles are in the cards.

The long list of smaller-format retailers that have gone from “hot” to “not”
includes, but is not “ ​Limited ​” to: Hot Topic, Quiksilver, Aeropostale, J. Crew, Express, and
essentially all of Ascena’s brands. Even cleats and clubs aren’t safe: Sports Authority went so
bankrupt that creditors chose to liquidate. 6 And it’s not as if these stores are closing into a
supply-constrained market - Jones Lang LaSalle notes the U.S. has as much retail space per capita
as “ ​France, Denmark, Finland, Portugal, Spain, Italy, and Germany combined.”

None of this is to say that Seritage couldn’t work. But imaginably, the counterparty (Simon?) to
those other downsizing retailers’ leases probably also wants to redevelop space… implying what for
pricing and utilization in an oversupplied market with shrinking demand? This system-level view
raises critical questions I consider “too hard” - i.e. unknowable and/or unpredictable. So I passed
Seritage by in favor of a much more clearly underpriced REIT I actually had an angle on. (A small
position and not my favorite idea, but in an expensive market, I’ll take value where I can find it.)


//

Most of the time, systems thinking helps you avoid dangerous pitfalls - take the standard B-school
example of terminal growth rates - if you assume a business will grow at 20% forever, ​you’re gonna
have a bad time, because SnackApp can’t be bigger than global GDP in 2035, it just can’t, I don’t
care how many selfies your cousin’s cat is sharing on it. Math says no. At a more sophisticated
level, incorporating systems thinking can help bottom-up value investors mesh deep business
understanding with the broader industry and customer context to obtain a more reasonable,
realistic set of expectations for how business performance will evolve over time.

That said, sharp thinkers reading between the lines of my first two examples might also spot a
categorically attractive fishing pond for potential ideas. I came close with one line on the second
page, but didn’t explicitly spell it out, because where would be the fun in that? ​If you’re up for
the challenge, email me with your thoughts.

If you make a valiant attempt, I’ll share my
hypothesis… and if you mention that you tweeted/LinkedIn-ed/emailed/otherwise shared this
paper, I may even throw in an actionable small-cap investment idea related to my premise. :)



(Sidenote from OP: DO NOT send your thoughts on the issue to me. The authors (Samir Patel) email can, I think, be found in the link. If it cannot, feel free to message me for his e-mail address.) "
2,2017-01-10,Charuru,25,My thoughts on competition to NVDA in GPUs,"If you're interested in investing in NVDA check out my new subreddit /r/NVDA_Stock. It's shamelessly inspired by /r/AMD_Stock You can read and discuss this article here: https://www.reddit.com/r/NVDA_Stock/comments/5n6ohq/nvidias_gpu_revenue_is_secure_for_the_next_year


**Nvidia's GPU revenue is secure for the next year, there is still no competition.**

There are many bears that are concerned about competition from AMD's Vega in both gaming and deep learning. I took the time to learn about Vega and study the state of the competition seriously. My conclusion is that competition is **non-existent**.



**Gaming**

Polaris failed to make an impact as we saw last quarter with NVIDIA's record revenues and AMD's very weak revenues. Coming a full year later after the launch of Pascal, Vega is expected to hit around GP104 performance. This level of performance is probably too low. GP104 was top tier in 2016 but will be merely mid-tier in 2017. The 1080ti will replace the 1080 as the gaming flagship for NVIDIA and Vega will only work as mid-tier competition. NVIDIA should also launch their Pascal refresh or Volta cards by fall of this year. If that's the case, then GP104 will fall to mid-low tier, not mid tier, and Vega will only compete on the budget level. As a result NVIDIA will once again face no competition in most of the market for gamers. 



**Deep learning**

In deep learning competition is again non-existent. Many people have pointed to Google also offering AMD cards in their cloud as a sign of validation for AMD as a deep learning competitor. But this is untrue and a media scam, or worse yet, some kind of scheme to fool investors.

https://cloudplatform.googleblog.com/2016/11/announcing-GPUs-for-Google-Cloud-Platform.html

Read the google cloud announcement directly instead of reading an ignorant regurgitation from financial news outlets.

&gt; Google Cloud will offer AMD FirePro S9300 x2 that supports powerful, GPU-based remote workstations. We'll also offer NVIDIA® Tesla® P100 and K80 GPUs for deep learning, AI and HPC applications that require powerful computation and analysis. GPUs are offered in passthrough mode to provide bare metal performance. Up to 8 GPU dies can be attached per VM instance including custom machine types.

As we can see, only NVIDIA cards are being offered for deep learning. AMD cards are only used for remote workstations, an old and uninteresting usecase. AMD has no competition for deep learning as of yet. The FirePro cards are offered at extreme discounts to the NVIDIA equivalents, we're talking about prices that are only 1/2 to 1/4 as much. Yet Quadro and Tesla dominate the market vs FirePro. In any case, NVIDIA is not interested in engaging in a race to the bottom with AMD and the customer base for those products are generally not so price sensitive.

AMD also announced a Vega based ""Instinct"" deep learning platform. Though 5 years late, AMD is hoping to start the catch up with the launch of Vega. There are 2 questions to think about with Instinct in respect to whether or not it'll be successful in deep learning.

Question 1 is the hardware, is it competitive with P100? Question 2 is the software, can the CUDA moat of NVIDIA be breached?



**THE HARDWARE**

Reference article: http://www.anandtech.com/show/10905/amd-announces-radeon-instinct-deep-learning-2017

Hardware-wise, NVIDIA has a feature advantage. NVIDIA's cards are optimized for ""deep learning operations"" while AMD is not. But let's be clear here, ""deep learning operations"" is marketing for INT8. Most deep learning today is done on FP16. P100 is ""10x"" faster than Maxwell partly based on improvements to FP16 deep learning operations and is today far ahead of any competition.

In the future, AMD is advertising better support for FP16, bringing them up to parity with NVIDIA in that regard. But NVIDIA is moving ahead with support for INT8 operations, an even faster way of doing deep learning than FP16. INT8 is however useful only in some usecases.

&gt; Deep learning research has found that trained deep neural networks can be applied to inference using reduced precision arithmetic, with minimal impact on accuracy. These instructions allow rapid computation on packed low-precision vectors. Tesla P4 is capable of a peak 21.8 INT8 TOP/s (Tera-Operations per second).

Investors should be careful to note that this is only NVIDIA marketing, INT8's usefulness in the real world has not yet been proven. In this case, hardware for INT8 did not exist prior to Pascal and it will be with the expansion of Pascal into the market that real applications for INT8 will be written. However if NVIDIA is correct (and I think they are), INT8 represents yet another generationally important hardware feature that NVIDIA has over AMD.

But let's ignore this for now and talk about competition in the more traditional FP16.

AMD is offering a Polaris, Fiji, and Vega based solution. 

Anandtech is politely saying that Polaris and Fiji are generally worse than the NVIDIA options.

&gt; The MI6 and MI8 will be going up against NVIDIA’s P4 and P40 accelerators. AMD’s cards don’t directly line-up against the NVIDIA cards in power consumption or expected performance, so the competitive landscape is somewhat broad, but those are the cards AMD will need to dethrone in the inference landscape.

I'll be more realistic and say right away that simply means AMD is completely non competitive for the Polaris and Fiji products, especially considering the relative price inelasticity of deep learning customers and NVIDIA's lead in CUDA (which I'll get to in section 2).

More interesting is MI25, the upcoming Vega based product. It's got new architectural improvements of uncertain value. We simply don't know how it will compete against P100.

&gt; As AMD’s sole training card, the MI25 will be going up against NVIDIA’s flagship accelerator, the Tesla P100. And as opposed to the inference cards, this has the potential to be a much closer fight. AMD has parity on packed instructions, with performance that on paper would exceed the P100. AMD has yet to fully unveil what Vega can do – we have no idea what “NCU” stands for or what AMD’s “high bandwidth cache and controller” are all about – but on the surface there’s the potential for the kind of knock-down fight at the top that makes for an interesting spectacle. And for AMD the stakes are huge; even if they can’t necessarily win, being able to price the MI25 even remotely close to the P100 would give them huge margins. More practically speaking, it means they could afford to significantly undercut NVIDIA in this space to capture market share while still making a tidy profit.

Anandtech is full of optimism for the MI25. I look at it much more critically. Even if the MI25 is competitive on the hardware level it is probably too late. Like in the gaming market, it comes a full year after Pascal and Volta-based deep learning chips are probably going to be announced before their release. This year's GTC (GPU Technology Conference), the annual NVIDIA hosted conference should see the announcement of Volta based V100 for deep learning. Pascal was announced at last year's GTC.

Based on this release cadence, AMD looks at least a year behind in technology even ignoring INT8. 


**The Software, CUDA Moat.**

A lot of investors have heard of CUDA and how important it is in deep learning. But it doesn't seem like its importance is sufficiently stressed seeing as how many bears are still out there talking about competition. CUDA has already won, there is no war, the war is over. NVIDIA's proprietary platform is as dominant as windows is over Linux. CUDA is easier to use, have a vastly bigger community, resources, tooling, love and support from everyone. There is basically no alternative. The idea of OpenCL winning vs CUDA in 2017 is as farcical as the idea of Linux winning over Windows on the desktop in 2017. So long as people love CUDA they will stick with NVIDIA.

AMD is going to attempt to breach the CUDA moat with the Boltzmann Project. It's a project to poorly port CUDA code to AMD compatible OpenCL code. To me this sounds almost a bit delusional... but I'll talk about it anyway. Has any software platform ever won by creating an emulator / port layer for another platform's user apps? Doesn't such a thing just send the signal that the winning platform has, in fact, won?

The Boltzmann Project is of dubious quality technology wise. I read in many places that the output is garbage. But just humoring it, we have to raise the question why anyone would want to port from a popular and great environment like CUDA to a crappy one like OpenCL in the first place. The only answer is, they don't want NVIDIA to get a monopoly on deep learning (it's too late NVDA already has a monopoly) and want to support AMD to help it become a competitor.

The only problem is such logic is that computer scientists are not fanboys. People generally don't spend weeks of work time to help out a company for the purpose of charity. They want to get stuff done, make software, get their AIs to make recommendations, help translate languages, create machine music, make medical diagnosis, whatever it is that they're doing. 

Is it possible for AMD to make ground in the CUDA moat? I can't dismiss it out of hand, if AMD makes an unrealistic, absurd amount of investment in that effort, I can see them making progress. [It's just extremely unlikely, and everyone knows it.](https://www.reddit.com/r/MachineLearning/comments/4m417z/amd_polaris/d3sholx/
)

In conclusion, there are no concerns about competition vis-a-vis AMD. But that doesn't mean there isn't a bear case for the NVDA stock. Stick around /r/NVDA_Stock and check out some other posts.


"
3,2017-01-06,AlphaQ69,1,Can someone help provide resources to me that I can learn options trading - practicing and learning to a point to trade live,"I work in finance on the real estate side so I'm knowledgeable about that area. I have a small portfolio of buy and hold stocks. I've never done options trading but it's something I'm interested in learning. 

I would like to dive into a few companies and really get knowledgeable about them and practice options trading on them for 3-6 months and see my performance. If I have successful results, then I'd like to start trading with real money. 

Does anyone have any websites, articles, videos, etc that dive deep into options trading?

Any recommended brokers to use to practice trading and doing live trading?

And useful apps for iOS (iPad mac or iPhone) ?

Would be greatly appreciate!"
4,2017-05-24,reddit1977,81,Is there AI today that is successful at day trading?,I've been watching videos about AI and this man Kai Fu Lee said something about deep learning where machines are used to pick stocks. Is that really what I think is happening?
5,2017-05-14,cbfreder,263,"Engineer here: Why does the investment community think that ""deep learning"" (whatever that is) is such a game changer?","I have a Ph.D.; some people might call me a data scientist; and I work in a field where we occasionally consider using convolutional neural networks.

Machine learning has been around a long time, and there have only been the typical, incremental improvements. The only thing that's really changed in the past 10 years is that the hardware has gotten faster, allowing people to tackle slightly more complex problems.

What makes this such area so ripe for investment all of the sudden?

**edit:** Engineers, I don't need you to tell me why machine learning is useful or why I'm wrong about incremental change. I want to know what the investors think. That's why I asked the question here."
6,2017-05-14,jmonjazi,37,"Wow Mark Cuban Tells CNBC FANG Stocks Are Undervalued. Thinks deep learning changes the game. ""You don't even understand""",
7,2017-12-10,kolt54321,0,What Financial Ratios would you use in an Nvidia analysis?,"Obviously P/E comes to mind, being higher than the rest of the sector, and ratios with R&amp;D can be helpful in seeing how it can sustain future growth in AI and deep learning markets (as they are new fields), but what else would you look at? It doesn't have much competition in any of the markets it's currently in, so I'm at a bit of a loss."
8,2017-12-08,ron_leflore,164,I trained a deep learning model to find the best investments.,"It didn't work.

I occasionally see posts suggesting this (one by /u/craino the other day), so I thought I'd give it a try.

Here's the details:

I downloaded the Sharadar SF0 dataset from Quandl
https://www.quandl.com/data/SF0-Free-US-Fundamentals-Data/documentation/about

It has 63 indicators on 2000+ companies (they say).

I selected the indicators for each company  on 12-31-2015.

I then went and got 12-31-2015  and 12-31-2016 closing prices using Quandl WIKI EOD to compute the percent gain during 2016.
https://www.quandl.com/data/WIKI-Wiki-EOD-Stock-Prices/documentation/database-overview

I put together a table that looked like this


Ticker | 2016%Gain | ACCOCI | Assets | Assetsc | ASSETSNC | etc.
---|---|----|----|----|----|----
AAL | 11.49 |-4732|4.84|998|384
AAN | 43.43 |-5170|2.65|NA|NA

The table has 65 columns and 1072 rows.  Each row is a different company.  Each company has about 63 indicators, but some were not available for particular companies.

I tried to train a deep learning network on the data.  ""Training"" basically says try to find some type of relationship using the 63 indicators in the table to predict the 2016%Gain.

Here's how the model training went:
[training graph](https://i.imgur.com/eRAq9i4.png)

The way training works is that you split the 1072 rows into two parts: a training set (blue) and a validation set (orange).  You can see from the training graph that the blue training deviance (difference between actual 2016%Gain and  model's prediction of 2016%Gain) got substantially smaller, indicating better predictive value.  However, that had no effect on the validation deviance (orange), which is data that the training process is not using.  This says the model is just overfitting the data and has no predictive value.

I'm definitely not a pro at this, just learning.  It's possible that someone could come up with a model that works, but I'll say it isn't easy.

Some technical details for those who are interested:  
I used H2o's deep learning model to fit a regression model on the 2016%return.  The training frame was 75% of the data, validation frame 25% of data.  The hidden layer sizes were 200,100,50,25,10. I used 10000 epochs of training. The activation function was Rectifier with Dropout.  The input dropout ratio was 0.2 and the hidden layer dropouts were all 0.4.


"
